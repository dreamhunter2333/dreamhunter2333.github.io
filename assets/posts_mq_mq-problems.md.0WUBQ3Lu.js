import{_ as e,c as a,o as t,aj as l}from"./chunks/framework.BF2cowCO.js";const p=JSON.parse('{"title":"消息队列常见问题","description":"","frontmatter":{"title":"消息队列常见问题","date":"2021-03-17 22:45:31","categories":["mq"],"tags":["mq"]},"headers":[],"relativePath":"posts/mq/mq-problems.md","filePath":"posts/mq/mq-problems.md","lastUpdated":1719480458000}'),i={name:"posts/mq/mq-problems.md"},o=l('<h1 id="消息队列常见问题" tabindex="-1">消息队列常见问题 <a class="header-anchor" href="#消息队列常见问题" aria-label="Permalink to &quot;消息队列常见问题&quot;">​</a></h1><h2 id="大量消息在-mq-里积压" tabindex="-1">大量消息在 mq 里积压 <a class="header-anchor" href="#大量消息在-mq-里积压" aria-label="Permalink to &quot;大量消息在 mq 里积压&quot;">​</a></h2><p>临时紧急扩容：</p><ul><li>先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。</li><li>新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。</li><li>然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。</li><li>接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。 等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。</li></ul><h2 id="mq-中的消息过期失效" tabindex="-1">mq 中的消息过期失效 <a class="header-anchor" href="#mq-中的消息过期失效" aria-label="Permalink to &quot;mq 中的消息过期失效&quot;">​</a></h2><p>批量重导，手写脚本恢复数据</p><h2 id="mq-快写满了" tabindex="-1">mq 快写满了 <a class="header-anchor" href="#mq-快写满了" aria-label="Permalink to &quot;mq 快写满了&quot;">​</a></h2><p>消费一个丢弃一个，快速消费掉所有的消息，手写脚本恢复数据</p><p>RocketMQ 的方案</p><ul><li>提高消费并行度</li><li>批量方式消费</li><li>跳过非重要消息</li><li>优化每条消息消费过程</li></ul><h2 id="如何设计一个消息队列" tabindex="-1">如何设计一个消息队列 <a class="header-anchor" href="#如何设计一个消息队列" aria-label="Permalink to &quot;如何设计一个消息队列&quot;">​</a></h2><ul><li>分布式 可伸缩性</li><li>持久化</li><li>可用性</li><li>数据零丢失</li></ul>',12),r=[o];function s(m,n,c,u,q,d){return t(),a("div",null,r)}const _=e(i,[["render",s]]);export{p as __pageData,_ as default};
